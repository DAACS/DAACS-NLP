{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOdxrxxXD0xI"
   },
   "source": [
    "# Using Large Language Models for Content Analysis\n",
    "\n",
    "By Indira Sen\n",
    "\n",
    "In this notebook, you will:\n",
    "- download a text-based dataset which has *labeled* examples. For instance, a sentiment analysis dataset with tweets and their corresponding sentiment label.\n",
    "- you will define a function that calls a large language model to prompt it to label the dataset with predicted sentiment. You can do in this in a few different modes.\n",
    "- You will investigate the LLM's predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S225s9mKcRi"
   },
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ocV3c4Q0EbH6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from daacs.infrastructure.bootstrap import Bootstrap\n",
    "b = Bootstrap() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPYmMnn-KNXw"
   },
   "source": [
    "As an example, we will use the same PStance dataset.\n",
    "\n",
    "You could also try other types of labeling, e.g., with one of the first hate speech datasets, specifically: https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master\n",
    "\n",
    "from the paper '[Automated Hate Speech Detection and the Problem of Offensive Language](https://ojs.aaai.org/index.php/ICWSM/article/view/14955)' from 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "U1nNcbGZEdWD",
    "outputId": "b4527166-4c2f-4fa3-df9e-a837b0b0575b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden is looking to gather votes from unsu...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check out the latest podcast conversation betw...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you Secretary Clinton for your endorseme...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happening now: @JoeBiden kicking off #Hispanic...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Mayor @KeishaBottoms for opening our...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet     Target   Stance\n",
       "0  Joe Biden is looking to gather votes from unsu...  Joe Biden  AGAINST\n",
       "1  Check out the latest podcast conversation betw...  Joe Biden    FAVOR\n",
       "2  Thank you Secretary Clinton for your endorseme...  Joe Biden    FAVOR\n",
       "3  Happening now: @JoeBiden kicking off #Hispanic...  Joe Biden    FAVOR\n",
       "4  Thank you Mayor @KeishaBottoms for opening our...  Joe Biden    FAVOR"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f'{b.DATA_DIR}/pol/raw_train_biden.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T18:52:03.888737Z",
     "iopub.status.busy": "2023-10-24T18:52:03.888431Z",
     "iopub.status.idle": "2023-10-24T18:52:03.903107Z",
     "shell.execute_reply": "2023-10-24T18:52:03.902076Z",
     "shell.execute_reply.started": "2023-10-24T18:52:03.888711Z"
    },
    "id": "JrY2xwn-FFer",
    "outputId": "50b7457e-def4-4254-afe7-082f019f2cf3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-17b66613-6757-45cf-ad39-bcb2912a3ea3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden is looking to gather votes from unsu...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check out the latest podcast conversation betw...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you Secretary Clinton for your endorseme...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happening now: @JoeBiden kicking off #Hispanic...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Mayor @KeishaBottoms for opening our...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>Call me stubborn but I just dont think I wanna...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Crazy liberals lol progressive policies work b...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>Lots of students @UWMadison awaiting @JoeBiden...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>Other than the terrible grammar, is #Biden jus...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>#Biden2020. Why dont the President and his sta...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5806 rows × 3 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17b66613-6757-45cf-ad39-bcb2912a3ea3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-17b66613-6757-45cf-ad39-bcb2912a3ea3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-17b66613-6757-45cf-ad39-bcb2912a3ea3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-682c498f-2dab-4824-b4df-0b403002420a\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-682c498f-2dab-4824-b4df-0b403002420a')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-682c498f-2dab-4824-b4df-0b403002420a button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  Tweet     Target   Stance\n",
       "0     Joe Biden is looking to gather votes from unsu...  Joe Biden  AGAINST\n",
       "1     Check out the latest podcast conversation betw...  Joe Biden    FAVOR\n",
       "2     Thank you Secretary Clinton for your endorseme...  Joe Biden    FAVOR\n",
       "3     Happening now: @JoeBiden kicking off #Hispanic...  Joe Biden    FAVOR\n",
       "4     Thank you Mayor @KeishaBottoms for opening our...  Joe Biden    FAVOR\n",
       "...                                                 ...        ...      ...\n",
       "5801  Call me stubborn but I just dont think I wanna...  Joe Biden  AGAINST\n",
       "5802  Crazy liberals lol progressive policies work b...  Joe Biden  AGAINST\n",
       "5803  Lots of students @UWMadison awaiting @JoeBiden...  Joe Biden    FAVOR\n",
       "5804  Other than the terrible grammar, is #Biden jus...  Joe Biden  AGAINST\n",
       "5805  #Biden2020. Why dont the President and his sta...  Joe Biden  AGAINST\n",
       "\n",
       "[5806 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZkHBqiqKkiu"
   },
   "source": [
    "## 2. Make the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KXVKWqs0FP41"
   },
   "outputs": [],
   "source": [
    "def make_prompt(task, options, instance, **kwargs):\n",
    "    options_str = '' # options ---> all possible labels\n",
    "    for i in range(len(options)):\n",
    "        options_str = options_str + ' %d) %s' %(i+1, options[i])\n",
    "    prompt = 'Given a piece of text, you have to label whether it is %s or not.\\\n",
    "    Please return one of the following options with only the text and no number:%s.'\\\n",
    "    %(task, options_str)\n",
    "\n",
    "    if kwargs['zero_shot']:\n",
    "        return prompt + ' What is the label of this text: \"' + instance+ '\"'\n",
    "    else: # for few-shot\n",
    "        examples_str = ''\n",
    "    for example in kwargs['examples']:\n",
    "        examples_str = examples_str + 'text: %s, label: %s\\n' %(example[0], example[1])\n",
    "    return prompt + ' Here are some examples of instances and their labels:\\\n",
    "    \\n%sWhat is the label of this text: ' %(examples_str) + instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MGfCQGBMIpum",
    "outputId": "1e92fbd6-85d8-440a-b30d-392d4d2c9ed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, this was true yesterday and it's also true now: Biden is an idiot\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = 'in favor of Joe Biden'\n",
    "options = ['FAVOR', 'AGAINST']\n",
    "examples = [] # the first two instances of the dataset are used as few-shot examples\n",
    "for _, row in data.iterrows():\n",
    "    examples.append([row['Tweet'], row['Stance']])\n",
    "    if len(examples) == 2:\n",
    "        break\n",
    "instance = \"Ugh, this was true yesterday and it's also true now: Biden is an idiot\" # you can replace this with instances in the dataset\n",
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "3FOxFMXgJPL8",
    "outputId": "b660d289-7177-4086-e5c8-5ec38f659e9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given a piece of text, you have to label whether it is in favor of Joe Biden or not.    Please return one of the following options with only the text and no number: 1) FAVOR 2) AGAINST. What is the label of this text: \"Ugh, this was true yesterday and it\\'s also true now: Biden is an idiot\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prompt(task, options, instance, zero_shot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T19:20:11.613729Z",
     "iopub.status.busy": "2023-10-24T19:20:11.612786Z",
     "iopub.status.idle": "2023-10-24T19:20:11.618824Z",
     "shell.execute_reply": "2023-10-24T19:20:11.617908Z",
     "shell.execute_reply.started": "2023-10-24T19:20:11.613694Z"
    },
    "id": "NLfOjnu0JkiG",
    "outputId": "90033684-2f2c-4e7c-82a1-a1d7aef73bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a piece of text, you have to label whether it is in favor of Joe Biden or not.    Please return one of the following options with only the text and no number: 1) FAVOR 2) AGAINST. Here are some examples of instances and their labels:    \n",
      "text: Joe Biden is looking to gather votes from unsuspecting voters. One must remember, Good Ole Boy Joe supported a Grand Wizard of the KKK. Joe cannot deny it., label: AGAINST\n",
      "text: Check out the latest podcast conversation between @JoeBiden and @AndrewYang. #HeresTheDeal #UnitedForJoe #BarnstormersForAmerica #ITrustJoe, label: FAVOR\n",
      "What is the label of this text: Ugh, this was true yesterday and it's also true now: Biden is an idiot\n"
     ]
    }
   ],
   "source": [
    "print(make_prompt(task, options, instance, zero_shot = False, examples = examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ockSQky4K_LL"
   },
   "outputs": [],
   "source": [
    "prompt = make_prompt(task, options, instance, zero_shot = False, examples = examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0j-ImkaKoDT"
   },
   "source": [
    "## 3. Call the LLM with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "soEH9rZjNH-K"
   },
   "outputs": [],
   "source": [
    "runs = 3 # specify how many labels we want per instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xozoA69-L9bW"
   },
   "source": [
    "We will try this with an open source model like Flan-T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T18:52:05.286521Z",
     "iopub.status.busy": "2023-10-24T18:52:05.286215Z",
     "iopub.status.idle": "2023-10-24T18:52:05.294825Z",
     "shell.execute_reply": "2023-10-24T18:52:05.293877Z",
     "shell.execute_reply.started": "2023-10-24T18:52:05.286495Z"
    },
    "id": "j-5UmNggMQZF"
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T18:52:05.296625Z",
     "iopub.status.busy": "2023-10-24T18:52:05.296330Z",
     "iopub.status.idle": "2023-10-24T18:53:30.559758Z",
     "shell.execute_reply": "2023-10-24T18:53:30.558734Z",
     "shell.execute_reply.started": "2023-10-24T18:52:05.296596Z"
    },
    "id": "-QJcHBPfLbZ0",
    "outputId": "2c9034cc-0e72-4c65-aa20-06ff6e006b81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour a cup of bolognese into a large bowl and add the pasta']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", max_new_tokens = 500)\n",
    "model.cuda()\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\",\n",
    "                   return_tensors=\"pt\").to(\"cuda:0\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mhPmRMjgqpj"
   },
   "source": [
    " The code to prompt Flan-T5 is a bit complex.\n",
    "\n",
    " We use the Hugging Face Transformers library to perform sequence-to-sequence (seq2seq) language modeling with a pre-trained model called \"google/flan-t5-xl.\n",
    "\n",
    "- AutoModelForSeq2SeqLM is used to load a pre-trained seq2seq model.\n",
    "\n",
    "- AutoTokenizer is used to load the tokenizer associated with the model.\n",
    "Load the pre-trained model and tokenizer:\n",
    "\n",
    "The code loads a pre-trained sequence-to-sequence model named \"google/flan-t5-xl\" using AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\"). This model is a variant of T5 (Text-to-Text Transfer Transformer) architecture.\n",
    "\n",
    "We then create a tokenizer with a specified maximum number of new tokens:\n",
    "\n",
    "- The code creates a tokenizer for the \"google/flan-t5-xl\" model using AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", max_new_tokens=500).\n",
    "\n",
    "This tokenizer is configured to handle sequences with a maximum of 500 additional tokens beyond the original text.\n",
    "\n",
    "We then move the model to the GPU (CUDA):\n",
    "\n",
    "model.cuda() moves the loaded model to the GPU for faster inference if a compatible GPU is available. It uses the \"cuda:0\" device.\n",
    "\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\").to(\"cuda:0\") tokenizes the input text \"A step by step recipe to make bolognese pasta:\" using the tokenizer. The return_tensors=\"pt\" option returns PyTorch tensors. The resulting tokenized input is then moved to the GPU.\n",
    "\n",
    "We then generate a sequence from the model:\n",
    "\n",
    "outputs = model.generate(**inputs) generates a sequence based on the tokenized input using the loaded model. The generate method takes the tokenized input as input and produces a sequence of output tokens.\n",
    "Decode and print the generated sequence:\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True) decodes the generated output tokens into text, skipping any special tokens that are not part of the final result.\n",
    "\n",
    "In summary, this code loads a pre-trained seq2seq model, tokenizes an input text, generates a sequence based on the input using the model, and then prints the generated text. It uses the \"google/flan-t5-xl\" model, which is a large T5 variant suitable for various text-to-text tasks. The code is designed for GPU acceleration for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T18:53:30.562021Z",
     "iopub.status.busy": "2023-10-24T18:53:30.561161Z",
     "iopub.status.idle": "2023-10-24T18:53:31.172022Z",
     "shell.execute_reply": "2023-10-24T18:53:31.171196Z",
     "shell.execute_reply.started": "2023-10-24T18:53:30.561984Z"
    },
    "id": "SuDH3A4kMaJm"
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "for n in range(0, runs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    responses.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T18:53:31.173431Z",
     "iopub.status.busy": "2023-10-24T18:53:31.173171Z",
     "iopub.status.idle": "2023-10-24T18:53:31.179994Z",
     "shell.execute_reply": "2023-10-24T18:53:31.179057Z",
     "shell.execute_reply.started": "2023-10-24T18:53:31.173409Z"
    },
    "id": "nkS7yBv4OBQr",
    "outputId": "2a200717-23ac-46a2-df06-9f15a0902396"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGAINST', 'AGAINST', 'AGAINST']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3NdIedL1ylD"
   },
   "source": [
    "Now we will try LLaMa, an LLM from Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T5NBVBt66eQc"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u4fejsq132n",
    "outputId": "7e8adf56-3f78-489b-d666-b2a66ae087dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
      "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "  Collecting setuptools>=42\n",
      "    Using cached setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
      "  Collecting scikit-build>=0.13\n",
      "    Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
      "  Collecting cmake>=3.18\n",
      "    Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "  Collecting ninja\n",
      "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "  Collecting distro (from scikit-build>=0.13)\n",
      "    Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "  Collecting packaging (from scikit-build>=0.13)\n",
      "    Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "  Collecting tomli (from scikit-build>=0.13)\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
      "    Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build\n",
      "    Creating /tmp/pip-build-env-skcbtxin/overlay/local/bin\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/ninja to 755\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/cmake to 755\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/cpack to 755\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/ctest to 755\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/wheel to 755\n",
      "    changing mode of /tmp/pip-build-env-skcbtxin/overlay/local/bin/distro to 755\n",
      "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "  lida 0.0.10 requires fastapi, which is not installed.\n",
      "  lida 0.0.10 requires kaleido, which is not installed.\n",
      "  lida 0.0.10 requires python-multipart, which is not installed.\n",
      "  lida 0.0.10 requires uvicorn, which is not installed.\n",
      "  tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
      "  Successfully installed cmake-3.28.1 distro-1.9.0 ninja-1.11.1.1 packaging-23.2 scikit-build-0.17.6 setuptools-69.0.3 tomli-2.0.1 wheel-0.42.0\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info\n",
      "  writing /tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  creating '/tmp/pip-modern-metadata-q4tzo_12/llama_cpp_python-0.1.78.dist-info'\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numpy==1.23.4\n",
      "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m260.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "    CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (0.3s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "    Command:\n",
      "      /tmp/pip-build-env-skcbtxin/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-skcbtxin/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-skcbtxin/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-skcbtxin/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- Using CUDA architectures: 52;61;70\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  -- Configuring done (2.7s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
      "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
      "  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
      "  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
      "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
      "  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
      "  [7/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
      "  [8/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
      "  [8/9] Install the project...\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
      "  -- Installing: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n",
      "\n",
      "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
      "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
      "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
      "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
      "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
      "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
      "  copying /tmp/pip-install-3z83vxis/llama-cpp-python_041dadf59808426f85227257c4953f9a/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
      "  copied 9 files\n",
      "  running build_ext\n",
      "  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copied 11 files\n",
      "  running install_data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n",
      "  running install_scripts\n",
      "  copied 0 files\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-ikear00v/.tmp-80psquu5/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'llama_cpp/__init__.py'\n",
      "  adding 'llama_cpp/libllama.so'\n",
      "  adding 'llama_cpp/llama.py'\n",
      "  adding 'llama_cpp/llama_cpp.py'\n",
      "  adding 'llama_cpp/llama_grammar.py'\n",
      "  adding 'llama_cpp/llama_types.py'\n",
      "  adding 'llama_cpp/py.typed'\n",
      "  adding 'llama_cpp/utils.py'\n",
      "  adding 'llama_cpp/server/__init__.py'\n",
      "  adding 'llama_cpp/server/__main__.py'\n",
      "  adding 'llama_cpp/server/app.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
      "  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811221 sha256=3dc9bf168d8d0736629ddf8f53591d6f00af22e3102b6ee06642b585edbfcd92\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-coh9rogr/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.9.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Removing file or directory /usr/local/bin/f2py\n",
      "      Removing file or directory /usr/local/bin/f2py3\n",
      "      Removing file or directory /usr/local/bin/f2py3.10\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.4.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "  changing mode of /usr/local/bin/f2py to 755\n",
      "  changing mode of /usr/local/bin/f2py3 to 755\n",
      "  changing mode of /usr/local/bin/f2py3.10 to 755\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.1.78\n",
      "    Uninstalling llama_cpp_python-0.1.78:\n",
      "      Removing file or directory /usr/local/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n",
      "      Removing file or directory /usr/local/bin/__pycache__/convert.cpython-310.pyc\n",
      "      Removing file or directory /usr/local/bin/convert-lora-to-ggml.py\n",
      "      Removing file or directory /usr/local/bin/convert.py\n",
      "      Removing file or directory /usr/local/lib/libggml_shared.so\n",
      "      Removing file or directory /usr/local/lib/libllama.so\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.78.dist-info/\n",
      "      Successfully uninstalled llama_cpp_python-0.1.78\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires openai, which is not installed.\n",
      "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
      "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.9.0\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
      "Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python==0.1.78\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "nW8aXHxK14AK"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GGML\"\n",
    "model_basename = \"llama-2-7b-chat.ggmlv3.q2_K.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "QIOT0AzH14C2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "jW6Z55fQ14GQ"
   },
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP2UNF2r2CFr",
    "outputId": "aeec57d0-9cab-40c3-9ee5-18f831644d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=2048\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "JRkLWhS_2CIU"
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression in python\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTBnJljk2CK3",
    "outputId": "6e206380-162d-4ad2-8f01-1f5c91c254c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression in python\n",
      "\n",
      "ASSISTANT:\n",
      "Wow! That's a great question! Linear regression is an important machine learning algorithm used to find the best-fitting line between two variables. In Python, you can use scikit-learn library which has built-in functions for linear regression such as SimpleLinearRegression and Ridge. Here are some examples of how you could implement these in Python:\n",
      "\n",
      "1. **Simple Linear Regression**\n",
      "```\n",
      "from sklearn.linear_model import SimpleLinearRegression\n",
      "X = [1, 2, 3, 4]\n",
      "y = [5, 7, 9, 10]\n",
      "reg = SimpleLinearRegression()\n",
      "reg.fit(X, y)\n",
      "print(\"Coefs:\", reg.coeff_)\n",
      "```\n",
      "This code fits a simple linear regression model to the data provided in `X` and `y`. The output will contain the coefficients of the linear regression equation.\n",
      "\n",
      "2. **Ridge Regression**\n",
      "```\n",
      "from sklearn.linear_model import RidgeRegressor\n",
      "X = [1, 2, 3, 4]\n",
      "y = [5, 7, 9, 10]\n",
      "reg = R\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "iUlU7uCC7isR",
    "outputId": "badec7c8-2b70-44c8-8785-2c57bc9b7cc2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Given a piece of text, you have to label whether it is in favor of Joe Biden or not.    Please return one of the following options with only the text and no number: 1) FAVOR 2) AGAINST. Here are some examples of instances and their labels:    \\ntext: Joe Biden is looking to gather votes from unsuspecting voters. One must remember, Good Ole Boy Joe supported a Grand Wizard of the KKK. Joe cannot deny it., label: AGAINST\\ntext: Check out the latest podcast conversation between @JoeBiden and @AndrewYang. #HeresTheDeal #UnitedForJoe #BarnstormersForAmerica #ITrustJoe, label: FAVOR\\nWhat is the label of this text: Ugh, this was true yesterday and it's also true now: Biden is an idiot\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try the political stance prompt again\n",
    "prompt = make_prompt(task, options, instance, zero_shot = False, examples = examples)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "CZB8Cp9V2COS"
   },
   "outputs": [],
   "source": [
    "prompt_template=f'''SYSTEM: Provide an honest answer.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aeyaioA2OpS",
    "outputId": "cc674293-669c-460a-95f2-c1640cf99c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for that specific piece of text would be AGAINST.\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=100, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  # echo=True\n",
    "                  )\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Huc-wJkRLiPD"
   },
   "source": [
    "Now do this for all the instances in your dataset.\n",
    "**Hint**: Use a loop over your dataframe. When doing few-shot labeling, make sure that the examples are not the same as the instance to be labeled.\n",
    "\n",
    "- Try both zero-shot and few-shot and compare their performance.\n",
    "- Try Flan-T5 small\n",
    "- Try to get the label from the LLM output. Is it always as expected and can it always be used as is for quantitative analysis?\n",
    "- At least for the first 50 instances in your dataset, use metrics like accuracy and F1 score to assess the performance of the LLMs against the true ground truth label.\n",
    "\n",
    "Bonus:\n",
    "- try varying the wording of the prompts\n",
    "- try giving an explicit definition of the task in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Kqmn8mhvKNX-",
    "outputId": "7ab10dd7-4ed1-48b7-c09d-8914fb71ddf9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-66a25b7a-2b03-4642-8ca7-ccade924861a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden is looking to gather votes from unsu...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check out the latest podcast conversation betw...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you Secretary Clinton for your endorseme...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happening now: @JoeBiden kicking off #Hispanic...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Mayor @KeishaBottoms for opening our...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a25b7a-2b03-4642-8ca7-ccade924861a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-66a25b7a-2b03-4642-8ca7-ccade924861a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-66a25b7a-2b03-4642-8ca7-ccade924861a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ffa9a61d-83f8-4929-9f62-ec75cd7c6db8\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffa9a61d-83f8-4929-9f62-ec75cd7c6db8')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ffa9a61d-83f8-4929-9f62-ec75cd7c6db8 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               Tweet     Target   Stance\n",
       "0  Joe Biden is looking to gather votes from unsu...  Joe Biden  AGAINST\n",
       "1  Check out the latest podcast conversation betw...  Joe Biden    FAVOR\n",
       "2  Thank you Secretary Clinton for your endorseme...  Joe Biden    FAVOR\n",
       "3  Happening now: @JoeBiden kicking off #Hispanic...  Joe Biden    FAVOR\n",
       "4  Thank you Mayor @KeishaBottoms for opening our...  Joe Biden    FAVOR"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subset = data.head(100)\n",
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqhC8DtEYh_t",
    "outputId": "9a235750-cae6-4230-b09b-f7c1458be698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:14<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # to help you keep track of how many instances have been labeled\n",
    "import time # to deal w/ rate limits; important for commercial models\n",
    "\n",
    "# another advantage of your own model is that you aren't rate limited\n",
    "all_responses = []\n",
    "for _, row in  tqdm(data_subset.iterrows(), total=data_subset.shape[0]):\n",
    "    prompt = make_prompt(task, options, zero_shot = True, instance = row['Tweet'])\n",
    "    responses = []\n",
    "    for n in range(0, runs):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        responses.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "    response_list = [row['Tweet'], row['Stance']]\n",
    "    response_list.extend(responses)\n",
    "    all_responses.append(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Zqhm_2VOYk8W",
    "outputId": "358369dc-d5b5-4f9f-f212-56b4f8909207"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8159d15e-3192-4f2f-bb49-1fa1c52b185c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hate speech</th>\n",
       "      <th>flant5_pred_1</th>\n",
       "      <th>flant5_pred_2</th>\n",
       "      <th>flant5_pred_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden is looking to gather votes from unsu...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check out the latest podcast conversation betw...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you Secretary Clinton for your endorseme...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happening now: @JoeBiden kicking off #Hispanic...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Mayor @KeishaBottoms for opening our...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8159d15e-3192-4f2f-bb49-1fa1c52b185c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8159d15e-3192-4f2f-bb49-1fa1c52b185c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8159d15e-3192-4f2f-bb49-1fa1c52b185c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-dacee19c-cec7-46cd-90c4-c78afab98025\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dacee19c-cec7-46cd-90c4-c78afab98025')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-dacee19c-cec7-46cd-90c4-c78afab98025 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               tweet hate speech  \\\n",
       "0  Joe Biden is looking to gather votes from unsu...     AGAINST   \n",
       "1  Check out the latest podcast conversation betw...       FAVOR   \n",
       "2  Thank you Secretary Clinton for your endorseme...       FAVOR   \n",
       "3  Happening now: @JoeBiden kicking off #Hispanic...       FAVOR   \n",
       "4  Thank you Mayor @KeishaBottoms for opening our...       FAVOR   \n",
       "\n",
       "  flant5_pred_1 flant5_pred_2 flant5_pred_3  \n",
       "0       AGAINST       AGAINST       AGAINST  \n",
       "1           NOT           NOT           NOT  \n",
       "2           NOT           NOT           NOT  \n",
       "3       AGAINST       AGAINST       AGAINST  \n",
       "4       AGAINST       AGAINST       AGAINST  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flant5_results = pd.DataFrame(all_responses, columns = ['tweet', 'hate speech', 'flant5_pred_1',\n",
    "                                      'flant5_pred_2',\n",
    "                                      'flant5_pred_3'])\n",
    "flant5_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0ex8ROHHl6E",
    "outputId": "a22aa28e-31d8-473c-f019-30e34111ef1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  1%|          | 1/100 [00:09<14:56,  9.05s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  2%|▏         | 2/100 [00:10<07:45,  4.75s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  3%|▎         | 3/100 [00:13<06:13,  3.85s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  4%|▍         | 4/100 [00:16<05:26,  3.40s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  5%|▌         | 5/100 [00:36<15:09,  9.58s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  6%|▌         | 6/100 [00:57<21:06, 13.47s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  7%|▋         | 7/100 [01:00<15:27,  9.97s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  8%|▊         | 8/100 [01:03<11:38,  7.59s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  9%|▉         | 9/100 [01:05<08:57,  5.90s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 10%|█         | 10/100 [01:09<08:17,  5.53s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 11%|█         | 11/100 [01:11<06:32,  4.41s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 12%|█▏        | 12/100 [01:21<08:51,  6.04s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 13%|█▎        | 13/100 [01:23<06:59,  4.82s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 14%|█▍        | 14/100 [01:26<06:01,  4.20s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 15%|█▌        | 15/100 [01:29<05:19,  3.75s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 16%|█▌        | 16/100 [01:31<04:31,  3.23s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 17%|█▋        | 17/100 [01:35<05:03,  3.66s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 18%|█▊        | 18/100 [01:50<09:23,  6.87s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 19%|█▉        | 19/100 [01:52<07:27,  5.52s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 20%|██        | 20/100 [01:54<05:54,  4.43s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 21%|██        | 21/100 [02:15<12:13,  9.28s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 22%|██▏       | 22/100 [02:32<15:17, 11.76s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 23%|██▎       | 23/100 [02:36<12:04,  9.41s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 24%|██▍       | 24/100 [02:39<09:22,  7.40s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 25%|██▌       | 25/100 [02:49<10:18,  8.25s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 26%|██▌       | 26/100 [03:07<13:46, 11.17s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 27%|██▋       | 27/100 [03:09<10:16,  8.44s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 28%|██▊       | 28/100 [03:14<08:54,  7.42s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 29%|██▉       | 29/100 [03:17<07:04,  5.98s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 30%|███       | 30/100 [03:18<05:29,  4.71s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 31%|███       | 31/100 [03:20<04:22,  3.80s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 32%|███▏      | 32/100 [03:23<03:55,  3.46s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 33%|███▎      | 33/100 [03:26<03:58,  3.55s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 34%|███▍      | 34/100 [03:35<05:34,  5.07s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 35%|███▌      | 35/100 [03:54<10:02,  9.27s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 36%|███▌      | 36/100 [04:11<12:09, 11.40s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 37%|███▋      | 37/100 [04:13<09:02,  8.60s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 38%|███▊      | 38/100 [04:22<09:13,  8.93s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 39%|███▉      | 39/100 [04:25<07:16,  7.16s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 40%|████      | 40/100 [04:37<08:37,  8.63s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 41%|████      | 41/100 [04:41<06:58,  7.10s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 42%|████▏     | 42/100 [04:43<05:18,  5.49s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 43%|████▎     | 43/100 [04:44<04:07,  4.34s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 44%|████▍     | 44/100 [04:46<03:21,  3.59s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 45%|████▌     | 45/100 [05:08<08:15,  9.01s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 46%|████▌     | 46/100 [05:10<06:21,  7.06s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 47%|████▋     | 47/100 [05:21<07:06,  8.04s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 48%|████▊     | 48/100 [05:22<05:19,  6.14s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 49%|████▉     | 49/100 [05:32<06:11,  7.29s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 50%|█████     | 50/100 [05:34<04:47,  5.75s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 51%|█████     | 51/100 [05:36<03:46,  4.61s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 52%|█████▏    | 52/100 [05:39<03:10,  3.96s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 53%|█████▎    | 53/100 [05:42<02:50,  3.63s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 54%|█████▍    | 54/100 [05:45<02:35,  3.38s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 55%|█████▌    | 55/100 [05:46<02:10,  2.90s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 56%|█████▌    | 56/100 [05:50<02:12,  3.02s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 57%|█████▋    | 57/100 [05:56<02:51,  4.00s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 58%|█████▊    | 58/100 [05:58<02:19,  3.31s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 59%|█████▉    | 59/100 [06:04<02:57,  4.33s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 60%|██████    | 60/100 [06:24<06:01,  9.03s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 61%|██████    | 61/100 [06:27<04:34,  7.04s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 62%|██████▏   | 62/100 [06:35<04:45,  7.52s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 63%|██████▎   | 63/100 [06:45<05:00,  8.11s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 64%|██████▍   | 64/100 [06:48<03:58,  6.62s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 65%|██████▌   | 65/100 [06:51<03:10,  5.44s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 66%|██████▌   | 66/100 [06:52<02:26,  4.32s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 67%|██████▋   | 67/100 [07:01<03:08,  5.73s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 68%|██████▊   | 68/100 [07:11<03:41,  6.92s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 69%|██████▉   | 69/100 [07:14<02:52,  5.56s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 70%|███████   | 70/100 [07:15<02:14,  4.49s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 71%|███████   | 71/100 [07:19<02:05,  4.32s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 72%|███████▏  | 72/100 [07:39<04:06,  8.81s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 73%|███████▎  | 73/100 [07:41<03:02,  6.77s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 74%|███████▍  | 74/100 [07:42<02:17,  5.27s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 75%|███████▌  | 75/100 [07:45<01:47,  4.30s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 76%|███████▌  | 76/100 [07:49<01:48,  4.50s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 77%|███████▋  | 77/100 [07:52<01:26,  3.77s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 78%|███████▊  | 78/100 [08:09<02:50,  7.77s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 79%|███████▉  | 79/100 [08:17<02:48,  8.00s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 80%|████████  | 80/100 [08:27<02:52,  8.64s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 81%|████████  | 81/100 [08:31<02:18,  7.30s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 82%|████████▏ | 82/100 [08:55<03:37, 12.09s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 83%|████████▎ | 83/100 [08:57<02:37,  9.28s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 84%|████████▍ | 84/100 [09:02<02:06,  7.93s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 85%|████████▌ | 85/100 [09:11<02:03,  8.21s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 86%|████████▌ | 86/100 [09:16<01:42,  7.32s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 87%|████████▋ | 87/100 [09:24<01:34,  7.27s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 88%|████████▊ | 88/100 [09:28<01:18,  6.58s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 89%|████████▉ | 89/100 [09:30<00:57,  5.20s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 90%|█████████ | 90/100 [09:56<01:53, 11.35s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 91%|█████████ | 91/100 [09:59<01:19,  8.83s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 92%|█████████▏| 92/100 [10:01<00:54,  6.83s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 93%|█████████▎| 93/100 [10:16<01:05,  9.32s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 94%|█████████▍| 94/100 [10:21<00:47,  7.99s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 95%|█████████▌| 95/100 [10:26<00:35,  7.04s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 96%|█████████▌| 96/100 [10:29<00:23,  5.83s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 97%|█████████▋| 97/100 [10:33<00:15,  5.12s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 98%|█████████▊| 98/100 [10:49<00:17,  8.59s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 99%|█████████▉| 99/100 [10:52<00:06,  6.71s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "100%|██████████| 100/100 [10:56<00:00,  6.56s/it]\n"
     ]
    }
   ],
   "source": [
    "all_responses = []\n",
    "for _, row in  tqdm(data_subset.iterrows(), total=data_subset.shape[0]):\n",
    "    prompt = make_prompt(task, options, zero_shot = True, instance = row['Tweet'])\n",
    "    prompt_template=f'''SYSTEM: Provide an honest answer.\n",
    "\n",
    "    USER: {prompt}\n",
    "\n",
    "    ASSISTANT:\n",
    "    '''\n",
    "    responses = []\n",
    "    for n in range(0, runs):\n",
    "        response=lcpp_llm(prompt=prompt_template, max_tokens=100, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  # echo=True\n",
    "                  )\n",
    "        responses.append(response[\"choices\"][0][\"text\"])\n",
    "    response_list = [row['Tweet'], row['Stance']]\n",
    "    response_list.extend(responses)\n",
    "    all_responses.append(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "H9g2Ht0jIu6w",
    "outputId": "a8c55708-d675-4fca-fafc-259b664bc441"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5250ef00-7cfd-4a0a-b9c4-da787ab8cf81\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hate speech</th>\n",
       "      <th>llama_pred_1</th>\n",
       "      <th>llama_pred_2</th>\n",
       "      <th>llama_pred_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>#Biden Today down at The Hollywood Walk of fam...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1) FAVOR</td>\n",
       "      <td>1) FAVOR</td>\n",
       "      <td>1) FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>FACT: @JoeBiden introduced the first climate c...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1) FAVOR 2) AGAINST</td>\n",
       "      <td>1) FAVOR 2) AGAINST</td>\n",
       "      <td>1) FAVOR 2) AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>.@JoeBiden was born and raised in Scranton, Pe...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1) FACTS - This statement is factually incorre...</td>\n",
       "      <td>Honestly, I cannot provide a label for that t...</td>\n",
       "      <td>1. FACTUAL: This statement appears to be based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I put my full support behind the Joe Schiavoni...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1) FAVOR 2) AGENT</td>\n",
       "      <td>1) FAVOR</td>\n",
       "      <td>1) FAVOR 2) AGENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Outgoing Presidents leave letters to their suc...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1) FAVOR</td>\n",
       "      <td>The label for this text is \"FAVOR\".</td>\n",
       "      <td>1) FAVOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5250ef00-7cfd-4a0a-b9c4-da787ab8cf81')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5250ef00-7cfd-4a0a-b9c4-da787ab8cf81 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5250ef00-7cfd-4a0a-b9c4-da787ab8cf81');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-de20f29b-fca3-44df-8e41-58618b1ae9cd\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de20f29b-fca3-44df-8e41-58618b1ae9cd')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-de20f29b-fca3-44df-8e41-58618b1ae9cd button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                tweet hate speech  \\\n",
       "95  #Biden Today down at The Hollywood Walk of fam...       FAVOR   \n",
       "96  FACT: @JoeBiden introduced the first climate c...       FAVOR   \n",
       "97  .@JoeBiden was born and raised in Scranton, Pe...       FAVOR   \n",
       "98  I put my full support behind the Joe Schiavoni...       FAVOR   \n",
       "99  Outgoing Presidents leave letters to their suc...       FAVOR   \n",
       "\n",
       "                                         llama_pred_1  \\\n",
       "95                                          1) FAVOR    \n",
       "96                                1) FAVOR 2) AGAINST   \n",
       "97  1) FACTS - This statement is factually incorre...   \n",
       "98                                  1) FAVOR 2) AGENT   \n",
       "99                                          1) FAVOR    \n",
       "\n",
       "                                         llama_pred_2  \\\n",
       "95                                           1) FAVOR   \n",
       "96                                1) FAVOR 2) AGAINST   \n",
       "97   Honestly, I cannot provide a label for that t...   \n",
       "98                                           1) FAVOR   \n",
       "99                The label for this text is \"FAVOR\".   \n",
       "\n",
       "                                         llama_pred_3  \n",
       "95                                           1) FAVOR  \n",
       "96                                1) FAVOR 2) AGAINST  \n",
       "97  1. FACTUAL: This statement appears to be based...  \n",
       "98                                  1) FAVOR 2) AGENT  \n",
       "99                                           1) FAVOR  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_results = pd.DataFrame(all_responses, columns = ['tweet', 'hate speech', 'llama_pred_1',\n",
    "                                      'llama_pred_2',\n",
    "                                      'llama_pred_3'])\n",
    "llama_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDmykfjOLaJ9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
